{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Many-to-many NLP task. Part-of-speech tagging\n",
    "\n",
    "## Goal\n",
    "\n",
    "Your goal is to implement Neural Network for tagging the part-of-speech entities.\n",
    "\n",
    "## Submission\n",
    "\n",
    "Submission format is described at competition page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:00:01.453145Z",
     "iopub.status.busy": "2023-09-18T08:00:01.452764Z",
     "iopub.status.idle": "2023-09-18T08:00:01.458956Z",
     "shell.execute_reply": "2023-09-18T08:00:01.457734Z",
     "shell.execute_reply.started": "2023-09-18T08:00:01.453113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:00:02.845257Z",
     "iopub.status.busy": "2023-09-18T08:00:02.844559Z",
     "iopub.status.idle": "2023-09-18T08:00:03.583229Z",
     "shell.execute_reply": "2023-09-18T08:00:03.582223Z",
     "shell.execute_reply.started": "2023-09-18T08:00:02.845221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/romangaraev/Study Projects/pmldlcourse/labs/2024/lab6/train.csv')\n",
    "test = pd.read_csv('/Users/romangaraev/Study Projects/pmldlcourse/labs/2024/lab6/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:00:03.585809Z",
     "iopub.status.busy": "2023-09-18T08:00:03.585410Z",
     "iopub.status.idle": "2023-09-18T08:00:03.600476Z",
     "shell.execute_reply": "2023-09-18T08:00:03.599307Z",
     "shell.execute_reply.started": "2023-09-18T08:00:03.585772Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>entity_id</th>\n",
       "      <th>entity</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>It</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>is</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>true</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>that</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>his</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  entity_id entity   tag\n",
       "0            0          0     It  PRON\n",
       "1            0          1     is  VERB\n",
       "2            0          2   true   ADJ\n",
       "3            0          3   that   ADP\n",
       "4            0          4    his   DET"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:00:05.145618Z",
     "iopub.status.busy": "2023-09-18T08:00:05.145240Z",
     "iopub.status.idle": "2023-09-18T08:00:05.177975Z",
     "shell.execute_reply": "2023-09-18T08:00:05.176684Z",
     "shell.execute_reply.started": "2023-09-18T08:00:05.145588Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>entity_id</th>\n",
       "      <th>entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>another</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>setback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>yesterday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  sentence_id  entity_id     entity\n",
       "0   0            0          0         In\n",
       "1   1            0          1    another\n",
       "2   2            0          2    setback\n",
       "3   3            0          3  yesterday\n",
       "4   4            0          4          ,"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's divide dataset on train and validation. And split the dataframe according to random split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:00:29.555970Z",
     "iopub.status.busy": "2023-09-18T08:00:29.555268Z",
     "iopub.status.idle": "2023-09-18T08:00:29.581627Z",
     "shell.execute_reply": "2023-09-18T08:00:29.580711Z",
     "shell.execute_reply.started": "2023-09-18T08:00:29.555937Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "VALIDATION_RATIO = 0.2\n",
    "train_split, val_split = train_test_split(range(train['sentence_id'].max()), test_size=VALIDATION_RATIO, random_state=420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then split the original dataframe by ids that we splitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:00:31.255516Z",
     "iopub.status.busy": "2023-09-18T08:00:31.254436Z",
     "iopub.status.idle": "2023-09-18T08:00:31.349488Z",
     "shell.execute_reply": "2023-09-18T08:00:31.348405Z",
     "shell.execute_reply.started": "2023-09-18T08:00:31.255472Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataframe = train[train['sentence_id'].isin(train_split)]\n",
    "val_dataframe = train[train['sentence_id'].isin(val_split)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:03:24.456282Z",
     "iopub.status.busy": "2023-09-18T08:03:24.455196Z",
     "iopub.status.idle": "2023-09-18T08:03:24.463135Z",
     "shell.execute_reply": "2023-09-18T08:03:24.461861Z",
     "shell.execute_reply.started": "2023-09-18T08:03:24.456245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pos_tags = ['ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRT', 'PRON', 'VERB', '.', 'X']\n",
    "cat2idx = {tag: i for i, tag in enumerate(pos_tags)}\n",
    "idx2cat = {v: k for k, v in cat2idx.items()}\n",
    "\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For working with datasets more efficiently, let's create separate classes for datasets. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:03:59.348445Z",
     "iopub.status.busy": "2023-09-18T08:03:59.348067Z",
     "iopub.status.idle": "2023-09-18T08:03:59.548524Z",
     "shell.execute_reply": "2023-09-18T08:03:59.547557Z",
     "shell.execute_reply.started": "2023-09-18T08:03:59.348413Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'vocab' from 'nltk.text' (/opt/homebrew/lib/python3.11/site-packages/nltk/text.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m420\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vocab \u001b[38;5;28;01mas\u001b[39;00m Vocab\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPosTaggingDataset\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataframe: pd\u001b[38;5;241m.\u001b[39mDataFrame, vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, max_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'vocab' from 'nltk.text' (/opt/homebrew/lib/python3.11/site-packages/nltk/text.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(420)\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "class PosTaggingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, vocab = None, max_size=100):\n",
    "        self.dataframe = dataframe\n",
    "        self._preprocess()\n",
    "        self.vocab = vocab or self._create_vocab()\n",
    "\n",
    "    def _preprocess(self):\n",
    "        # fill missing values in entities\n",
    "        self.dataframe['entity'].fillna('', inplace=True)\n",
    "\n",
    "        # Fill missing tag to `other` - `X`\n",
    "        self.dataframe['tag'].fillna('X')\n",
    "\n",
    "        # Clean entities column\n",
    "        self.dataframe['entity'] = self.dataframe['entity'].str.lower()\n",
    "        \n",
    "        # Split the dataset, so that we will have \n",
    "        # full sentences and full tags by the same index\n",
    "        grouped_dataframe = self.dataframe.groupby(by='sentence_id')\n",
    "\n",
    "        self.sentences = grouped_dataframe['entity'].apply(list).to_list()\n",
    "        self.tags = grouped_dataframe['tag'].apply(list).to_list()\n",
    "    \n",
    "    def _create_vocab(self):\n",
    "        # creates vocabulary that is used for encoding \n",
    "        # the sequence of tokens (splitted sentence)\n",
    "        vocab = build_vocab_from_iterator(\n",
    "            self.sentences,\n",
    "            specials=special_symbols,\n",
    "            special_first=True,\n",
    "        )\n",
    "        vocab.set_default_index(UNK_IDX)\n",
    "        return vocab\n",
    "    \n",
    "    def _get_sentence(self, index: int) -> list:\n",
    "        # retrieves sentence from dataset by index\n",
    "        sent = self.sentences[index]\n",
    "        if self.vocab is None:\n",
    "            return sent\n",
    "        return self.vocab(sent)\n",
    "\n",
    "    def _get_labels(self, index: int) -> list:\n",
    "        # retrieves tags from dataset by index\n",
    "        tags = self.tags[index]\n",
    "        return [cat2idx.get(tag, cat2idx['X']) for tag in tags]\n",
    "    \n",
    "    def __getitem__(self, index) -> tuple[list, list]:\n",
    "        return self._get_sentence(index), self._get_labels(index)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sentences)\n",
    "\n",
    "# Create train dataset\n",
    "train_dataset = PosTaggingDataset(dataframe=train_dataframe)\n",
    "val_dataset = PosTaggingDataset(dataframe=val_dataframe, vocab=train_dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:04:00.395736Z",
     "iopub.status.busy": "2023-09-18T08:04:00.395046Z",
     "iopub.status.idle": "2023-09-18T08:04:05.863331Z",
     "shell.execute_reply": "2023-09-18T08:04:05.862306Z",
     "shell.execute_reply.started": "2023-09-18T08:04:00.395684Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create train dataset\n",
    "train_dataset = PosTaggingDataset(dataframe=train_dataframe)\n",
    "val_dataset = PosTaggingDataset(dataframe=val_dataframe, vocab=train_dataset.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are able to reate dataloader faster, because we created torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:08:26.908224Z",
     "iopub.status.busy": "2023-09-18T08:08:26.907869Z",
     "iopub.status.idle": "2023-09-18T08:08:26.919813Z",
     "shell.execute_reply": "2023-09-18T08:08:26.918764Z",
     "shell.execute_reply.started": "2023-09-18T08:08:26.908192Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "max_size = 50\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def collate_batch(batch: list):\n",
    "    # Collate list of samples into tensor batch\n",
    "    # As an input we have list of pair from dataset:\n",
    "    # [([ent1, ent2, ...], [tag1, tag2, ...]), ([ent1, ent2, ...], [tag1, tag2, ...]), ...]\n",
    "    # as an output, we want to have tensor of entities and tensor of tags \n",
    "    sentences_batch, postags_batch = [], []\n",
    "    for _sent, _postags in batch:\n",
    "        _sent_tensor = torch.Tensor(_sent)\n",
    "        _postags_tensor = torch.Tensor(_postags)\n",
    "        if len(_sent) > max_size:\n",
    "            sentences_batch.append(_sent_tensor[:max_size])\n",
    "            postags_batch.append(_postags_tensor[:max_size])\n",
    "        else:\n",
    "            sent_padding = torch.Tensor([1] * (max_size - len(_sent)))\n",
    "            tags_padding = torch.Tensor([cat2idx['X']] * (max_size - len(_sent)))\n",
    "\n",
    "            sentences_batch.append(torch.concat((_sent_tensor, sent_padding)))\n",
    "            postags_batch.append(torch.concat((_postags_tensor, tags_padding)))\n",
    "\n",
    "    # Remember, that if we want to perform many to many mapping with our network with recurrent units, \n",
    "    # we want pass first item from all sequences as first input, thus\n",
    "    # we want to have tensor with shape (max_size, ...., batch_size)\n",
    "    return torch.stack(sentences_batch, dim=0).int().T.to(device), torch.stack(postags_batch, dim=0).T.long().to(device)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:08:45.442091Z",
     "iopub.status.busy": "2023-09-18T08:08:45.441732Z",
     "iopub.status.idle": "2023-09-18T08:08:45.462464Z",
     "shell.execute_reply": "2023-09-18T08:08:45.461486Z",
     "shell.execute_reply.started": "2023-09-18T08:08:45.442060Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# just to check that all shapes are correct\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m      4\u001b[0m     inp, out \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(inp\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 13\u001b[0m, in \u001b[0;36mcollate_batch\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     11\u001b[0m sentences_batch, postags_batch \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _sent, _postags \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m---> 13\u001b[0m     _sent_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_sent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     _postags_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(_postags)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_sent) \u001b[38;5;241m>\u001b[39m max_size:\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "# just to check that all shapes are correct\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    inp, out = batch\n",
    "    print(inp.shape)\n",
    "    print(out.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network\n",
    "\n",
    "For the many-to-many or seq2seq netoworks, we want to have recurrent units in the network. This gives the ability for network to learn the hidden features and pass the knowledge from one token to other. \n",
    "\n",
    "### Embeddings\n",
    "\n",
    "For embeddings you can use `nn.Embedding` for creating your own features or use pretrained embedding (like GloVe or FastText or Bert).\n",
    "\n",
    "### Recurrent\n",
    "\n",
    "For processing sequences you can use recurrent units like `LSTM`.\n",
    "\n",
    "### Linear\n",
    "\n",
    "Add simple nn.Linear. ~~This is basic stuff what do you want~~\n",
    "\n",
    "### Regularization\n",
    "\n",
    "Remeber to set up Dropout and Batch Normalization for regularization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:15:59.837253Z",
     "iopub.status.busy": "2023-09-18T08:15:59.836884Z",
     "iopub.status.idle": "2023-09-18T08:15:59.846976Z",
     "shell.execute_reply": "2023-09-18T08:15:59.845829Z",
     "shell.execute_reply.started": "2023-09-18T08:15:59.837224Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTMPOSTagger(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim, \n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 dropout, \n",
    "                 pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers = n_layers, \n",
    "                            bidirectional = bidirectional,\n",
    "                            dropout = dropout if n_layers > 1 else 0)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        #pass text through embedding layer\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        #pass embeddings into LSTM\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        #outputs holds the backward and forward hidden states in the final layer\n",
    "        #hidden and cell are the backward and forward hidden and cell states at the final time-step\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * n directions]\n",
    "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #we use our outputs to make a prediction of what the tag should be\n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "        \n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "As for training you should take into account that the shape of your output and shape of the labels. Perform required transformations and use loss function that fits your task.\n",
    "\n",
    "> Do not forget about tqdm and logging, you want normal training not some unreadable ~~sht~~ logs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:18:44.904098Z",
     "iopub.status.busy": "2023-09-18T08:18:44.903728Z",
     "iopub.status.idle": "2023-09-18T08:18:44.920970Z",
     "shell.execute_reply": "2023-09-18T08:18:44.919987Z",
     "shell.execute_reply.started": "2023-09-18T08:18:44.904066Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "def train_one_epoch(\n",
    "    model,\n",
    "    loader,\n",
    "    optimizer,\n",
    "    loss_fn\n",
    "):\n",
    "    loop = tqdm(\n",
    "        enumerate(loader, 1),\n",
    "        total=len(loader),\n",
    "        desc=f\"Epoch {epoch}: train\",\n",
    "        leave=True,\n",
    "    )\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    total = 0\n",
    "    for i, batch in loop:\n",
    "        texts, labels = batch\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass and loss calculation\n",
    "        outputs = model(texts)\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "        \n",
    "        labels = labels.reshape(-1)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # optimizer run\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        loop.set_postfix({\"loss\": train_loss/total})\n",
    "\n",
    "def val_one_epoch(\n",
    "    model,\n",
    "    loader,\n",
    "    loss_fn,\n",
    "    best_so_far=0.0,\n",
    "    ckpt_path='best.pt'\n",
    "):\n",
    "    \n",
    "    loop = tqdm(\n",
    "        enumerate(loader, 1),\n",
    "        total=len(loader),\n",
    "        desc=f\"Epoch {epoch}: val\",\n",
    "        leave=True,\n",
    "    )\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # evaluation mode\n",
    "        for i, batch in loop:\n",
    "            texts, labels = batch\n",
    "\n",
    "            # forward pass and loss calculation\n",
    "            outputs = model(texts)\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "\n",
    "            labels = labels.reshape(-1)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            loop.set_postfix({\"loss\": val_loss/total, \"acc\": correct / total})\n",
    "        \n",
    "        if correct / total > best:\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "            return correct / total\n",
    "\n",
    "    return best_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:24:11.673902Z",
     "iopub.status.busy": "2023-09-18T08:24:11.673094Z",
     "iopub.status.idle": "2023-09-18T08:24:11.734966Z",
     "shell.execute_reply": "2023-09-18T08:24:11.734053Z",
     "shell.execute_reply.started": "2023-09-18T08:24:11.673866Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(train_dataset.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = len(pos_tags)\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "PAD_IDX = 1\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "\n",
    "model = BiLSTMPOSTagger(INPUT_DIM, \n",
    "                        EMBEDDING_DIM, \n",
    "                        HIDDEN_DIM, \n",
    "                        OUTPUT_DIM, \n",
    "                        N_LAYERS, \n",
    "                        BIDIRECTIONAL, \n",
    "                        DROPOUT, \n",
    "                        PAD_IDX).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:24:12.375499Z",
     "iopub.status.busy": "2023-09-18T08:24:12.374478Z",
     "iopub.status.idle": "2023-09-18T08:27:34.116975Z",
     "shell.execute_reply": "2023-09-18T08:27:34.115857Z",
     "shell.execute_reply.started": "2023-09-18T08:24:12.375455Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5957dee9eabb4dc087ac3f074092e5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0: train:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9c8d3a2380442dbee8475da3c7f398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0: val:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26a5752ca694ab3a01f7933b8824500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1: train:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfc99f73d734b1d8e35093fa9a80aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1: val:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9bb68bd0d884f4b915e21a2fa3c30e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2: train:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de87cc644b764398a0e1140454e6f67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2: val:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037d5f2f676c432ab5e956daf833ae28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3: train:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae8bc60150b42f8aacfbd49159c7b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3: val:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6597a395da9a4fc284fee62f64e4e992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4: train:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6e8631795f405696265790a989db0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4: val:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a18eb732674ed4a79272dab960cd11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5: train:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd92f4a6c7364e7dbe1de10a0d3e8c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5: val:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1668cb17cd9c4341a4bddfba36f1dbd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6: train:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb5fabf115948c1acce496ad845d423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6: val:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24364f9d00e34a4ab7076cbf524540b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7: train:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d44221ff4054c17bf6163374a84b530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7: val:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76145d4cedf41b282e1bd6cb4552b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8: train:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f5cc4c280f4171a8cc10a86e5c8db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8: val:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0156457d5d5402d84084bed45d421e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9: train:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f934fda80de46fbbd4c04a304c55758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9: val:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198c6411e357490cac5172acd4c5c2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10: train:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fb5f5ab444490e8507f1efdef8cd0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10: val:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db7092193264b90a333e1226833e34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11: train:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bde6636b2364c2eb7cc4a80d0ccf007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11: val:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63d296c7720421c8da7b443811648e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12: train:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae38e6245dd44319bdca36a4d76f22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12: val:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23776a5ee1564158826654d834f91fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13: train:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897f64484fb2431fb73cc6475de6460b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13: val:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d501212d05b4e3b98229a6fca74b8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14: train:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a450e7af9d43be8ad79ccf1cce58fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14: val:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2777cca7d1f4255a1893f8ef3615fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15: train:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69358a624ebf4a219e5e7fe1227e10ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15: val:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12abf6fef9514ee6a68ac164bd8d618e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16: train:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2fc4c16e0c040da9a2aae47145d9d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16: val:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6259664e7b64af0a9d37f39ce863507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17: train:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5b6feef9e742508e4dc3925ba24092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17: val:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a67b81081274e9292a679b3416ff571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18: train:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6c2605082c4c08a959bab91ad7d6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18: val:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7aec6246ae4bc3b311cf4f2ce19c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19: train:   0%|          | 0/361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f541d34d37ef413fbf7a09efa8eda362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19: val:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best = -float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, train_dataloader, optimizer, loss_fn)\n",
    "    best_so_far = val_one_epoch(model, val_dataloader, loss_fn, best_so_far=best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "\n",
    "Write prediction. That's it. No more instructions, you already made it 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:28:20.107494Z",
     "iopub.status.busy": "2023-09-18T08:28:20.107085Z",
     "iopub.status.idle": "2023-09-18T08:28:21.272345Z",
     "shell.execute_reply": "2023-09-18T08:28:21.271329Z",
     "shell.execute_reply.started": "2023-09-18T08:28:20.107463Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test['tag'] = 'X'\n",
    "test_dataset = PosTaggingDataset(test, vocab=train_dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:28:21.274793Z",
     "iopub.status.busy": "2023-09-18T08:28:21.274161Z",
     "iopub.status.idle": "2023-09-18T08:28:21.294821Z",
     "shell.execute_reply": "2023-09-18T08:28:21.293881Z",
     "shell.execute_reply.started": "2023-09-18T08:28:21.274757Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def collate_batch(batch: list):\n",
    "    sentences_batch, sentences_lengths = [], []\n",
    "    max_size = max([len(sent) for sent, _ in batch])\n",
    "    for _sent, _ in batch:\n",
    "        _sent_tensor = torch.Tensor(_sent)\n",
    "        sentences_lengths.append(len(_sent))\n",
    "\n",
    "        sent_padding = torch.Tensor([1] * (max_size - len(_sent)))\n",
    "        sentences_batch.append(torch.concat((_sent_tensor, sent_padding)))\n",
    "\n",
    "    return torch.stack(sentences_batch, dim=0).int().T.to(device), sentences_lengths\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=128, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:28:21.297045Z",
     "iopub.status.busy": "2023-09-18T08:28:21.296388Z",
     "iopub.status.idle": "2023-09-18T08:28:21.308636Z",
     "shell.execute_reply": "2023-09-18T08:28:21.307624Z",
     "shell.execute_reply.started": "2023-09-18T08:28:21.297010Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model,\n",
    "    loader,\n",
    "):\n",
    "    loop = tqdm(\n",
    "        enumerate(loader, 1),\n",
    "        total=len(loader),\n",
    "        desc=f\"Predictions\",\n",
    "        leave=True,\n",
    "    )\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # evaluation mode\n",
    "        for i, (texts, real_lengths) in loop:\n",
    "\n",
    "            # forward pass and loss calculation\n",
    "            outputs = model(texts)\n",
    "            \n",
    "            for idx, length in enumerate(real_lengths):\n",
    "                _, predicted = torch.max(outputs.data[:, idx], 1)\n",
    "                predictions += predicted[:length].detach().cpu().tolist()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:28:21.556273Z",
     "iopub.status.busy": "2023-09-18T08:28:21.555980Z",
     "iopub.status.idle": "2023-09-18T08:28:24.133156Z",
     "shell.execute_reply": "2023-09-18T08:28:24.132149Z",
     "shell.execute_reply.started": "2023-09-18T08:28:21.556248Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8df1e5c66c54d0fb91e8ad76dd1cff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predictions:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1, 4, 5, 5, 10, 5, 7, 5, 5, 9]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"best.pt\")\n",
    "model.load_state_dict(ckpt)\n",
    "\n",
    "predictions = predict(model, test_dataloader)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:28:24.136096Z",
     "iopub.status.busy": "2023-09-18T08:28:24.135242Z",
     "iopub.status.idle": "2023-09-18T08:28:25.033942Z",
     "shell.execute_reply": "2023-09-18T08:28:25.032807Z",
     "shell.execute_reply.started": "2023-09-18T08:28:24.136058Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = pd.Series(predictions).apply(lambda x: idx2cat[x])\n",
    "results.to_csv('submission.csv', index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T08:28:25.036048Z",
     "iopub.status.busy": "2023-09-18T08:28:25.035507Z",
     "iopub.status.idle": "2023-09-18T08:28:25.045347Z",
     "shell.execute_reply": "2023-09-18T08:28:25.044276Z",
     "shell.execute_reply.started": "2023-09-18T08:28:25.036000Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          ADP\n",
       "1          DET\n",
       "2         NOUN\n",
       "3         NOUN\n",
       "4            .\n",
       "          ... \n",
       "303020    NOUN\n",
       "303021     PRT\n",
       "303022    VERB\n",
       "303023    NOUN\n",
       "303024       .\n",
       "Length: 303025, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":85406,"databundleVersionId":9643625,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep Learning in Natural Language Processing\n\n## Goal\n\nYour goal is to implement Neural Network to classify Amazon Products reviews. \n\n## Submission\n\nSubmission format is described at competition page.","metadata":{}},{"cell_type":"markdown","source":"## Data preprocessing\n\nData preprocessing is an essential step in building a Machine Learning model and depending on how well the data has been preprocessed.\n\nIn NLP, text preprocessing is the first step in the process of building a model.\n\nThe various text preprocessing steps are:\n\n* Tokenization\n* Lower casing\n* Stop words removal\n* Stemming\n\nThese various text preprocessing steps are widely used for dimensionality reduction.\n\nFirst, let's read the input data and then perform preprocessing steps","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ntrain_dataframe = pd.read_csv('/kaggle/input/pmldl-week-4-helpfulness-of-amazon-reviews/train.csv')\ntest_dataframe = pd.read_csv('/kaggle/input/pmldl-week-4-helpfulness-of-amazon-reviews/test.csv')\n\ntrain_dataframe.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:10:22.364275Z","iopub.execute_input":"2024-09-26T23:10:22.364723Z","iopub.status.idle":"2024-09-26T23:10:23.370139Z","shell.execute_reply.started":"2024-09-26T23:10:22.364679Z","shell.execute_reply":"2024-09-26T23:10:23.368824Z"},"trusted":true},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"                                 Title Helpfulness  Score  \\\n0  Golden Valley Natural Buffalo Jerky         0/0    3.0   \n1                         Westing Game         0/0    5.0   \n2                         Westing Game         0/0    5.0   \n3                         Westing Game         0/0    5.0   \n4    I SPY A is For Jigsaw Puzzle 63pc         2/4    5.0   \n\n                                                Text              Category  \n0  The description and photo on this product need...  grocery gourmet food  \n1  This was a great book!!!! It is well thought t...            toys games  \n2  I am a first year teacher, teaching 5th grade....            toys games  \n3  I got the book at my bookfair at school lookin...            toys games  \n4  Hi! I'm Martine Redman and I created this puzz...            toys games  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Helpfulness</th>\n      <th>Score</th>\n      <th>Text</th>\n      <th>Category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Golden Valley Natural Buffalo Jerky</td>\n      <td>0/0</td>\n      <td>3.0</td>\n      <td>The description and photo on this product need...</td>\n      <td>grocery gourmet food</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Westing Game</td>\n      <td>0/0</td>\n      <td>5.0</td>\n      <td>This was a great book!!!! It is well thought t...</td>\n      <td>toys games</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Westing Game</td>\n      <td>0/0</td>\n      <td>5.0</td>\n      <td>I am a first year teacher, teaching 5th grade....</td>\n      <td>toys games</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Westing Game</td>\n      <td>0/0</td>\n      <td>5.0</td>\n      <td>I got the book at my bookfair at school lookin...</td>\n      <td>toys games</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I SPY A is For Jigsaw Puzzle 63pc</td>\n      <td>2/4</td>\n      <td>5.0</td>\n      <td>Hi! I'm Martine Redman and I created this puzz...</td>\n      <td>toys games</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"In the training data we have `4` features (`Title`, `Helpfulness`, `Score` and `Text`) with target category (`Category`). For the test features are the same, except for target column.\n\nFirst, let's write functions for preprocessing helpfulness and score feature in case we needed them.","metadata":{}},{"cell_type":"code","source":"arr = [obj.split('/') for obj in train_dataframe['Helpfulness']] \nans = []\nfor element in arr:\n    if float(element[1]) == 0:\n        element[1] = 1\n    ans.append((float(element[0]) / float(element[1])))\nprint(len(ans))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:10:23.372447Z","iopub.execute_input":"2024-09-26T23:10:23.373233Z","iopub.status.idle":"2024-09-26T23:10:23.435771Z","shell.execute_reply.started":"2024-09-26T23:10:23.373176Z","shell.execute_reply":"2024-09-26T23:10:23.433957Z"},"trusted":true},"outputs":[{"name":"stdout","text":"40000\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\ndef preprocess_score_inplace(df):\n    \"\"\"\n    Normalizes score to make it from 0 to 1.\n    \n    For now it is from 1.0 to 5.0, so natural choice\n    is to normalize by (f - 1.0)/4.0\n    \"\"\"\n    df['Score'] = (df['Score'] - 1.0) / 4.0\n    return df\n\ndef preprocess_helpfulness_inplace(df):\n    \"\"\"\n    Splits feature by '/' and normalize helpfulness to make it from 0 to 1\n    \n    The total number of assessments can be 0, so let's substitute it\n    with 1. The resulting helpfulness still will be zero but we\n    remove the possibility of division by zero exception.\n\n    Return value should be float\n    \"\"\"\n    # Write your code here\n    arr = [obj.split('/') for obj in df['Helpfulness']] \n    ans = []\n    for element in arr:\n        if float(element[1]) == 0:\n            element[1] = 1\n        ans.append((float(element[0]) / float(element[1])))\n    df['Helpfulness'] = ans\n    return df    ","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:10:23.437167Z","iopub.execute_input":"2024-09-26T23:10:23.437539Z","iopub.status.idle":"2024-09-26T23:10:23.449009Z","shell.execute_reply.started":"2024-09-26T23:10:23.437499Z","shell.execute_reply":"2024-09-26T23:10:23.447028Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"The two other features are both text. For simplicity, let's remove concatenate them so that we will have one full text feature. The resulting code is also a function.","metadata":{}},{"cell_type":"code","source":"def concat_title_text_inplace(df):\n    \"\"\"\n    Concatenates Title and Text columns together\n    \"\"\"\n    df['Text'] = df['Title'] + \" \" + df['Text']\n    df.drop('Title', axis=1, inplace=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:10:23.452212Z","iopub.execute_input":"2024-09-26T23:10:23.452728Z","iopub.status.idle":"2024-09-26T23:10:23.462110Z","shell.execute_reply.started":"2024-09-26T23:10:23.452677Z","shell.execute_reply":"2024-09-26T23:10:23.460817Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"Also, encode the target categories, so that the output is become an index","metadata":{}},{"cell_type":"code","source":"# define categories indices\ncat2idx = {\n    'toys games': 0,\n    'health personal care': 1,\n    'beauty': 2,\n    'baby products': 3,\n    'pet supplies': 4,\n    'grocery gourmet food': 5,\n}\n# define reverse mapping\nidx2cat = {\n    v:k for k,v in cat2idx.items()\n}","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:10:23.464137Z","iopub.execute_input":"2024-09-26T23:10:23.464554Z","iopub.status.idle":"2024-09-26T23:10:23.477865Z","shell.execute_reply.started":"2024-09-26T23:10:23.464514Z","shell.execute_reply":"2024-09-26T23:10:23.476411Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def encode_categories(df):\n    df['Category'] = df['Category'].apply(lambda x: cat2idx[x])\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:10:23.479217Z","iopub.execute_input":"2024-09-26T23:10:23.479651Z","iopub.status.idle":"2024-09-26T23:10:23.490545Z","shell.execute_reply.started":"2024-09-26T23:10:23.479606Z","shell.execute_reply":"2024-09-26T23:10:23.489154Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"Let's visualize our first stage of preprocessing.","metadata":{}},{"cell_type":"code","source":"train_copy = train_dataframe.head().copy()\n\nencode_categories(preprocess_score_inplace(preprocess_helpfulness_inplace(concat_title_text_inplace(train_copy))))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:10:23.492093Z","iopub.execute_input":"2024-09-26T23:10:23.492481Z","iopub.status.idle":"2024-09-26T23:10:23.521932Z","shell.execute_reply.started":"2024-09-26T23:10:23.492433Z","shell.execute_reply":"2024-09-26T23:10:23.520635Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   Helpfulness  Score                                               Text  \\\n0          0.0    0.5  Golden Valley Natural Buffalo Jerky The descri...   \n1          0.0    1.0  Westing Game This was a great book!!!! It is w...   \n2          0.0    1.0  Westing Game I am a first year teacher, teachi...   \n3          0.0    1.0  Westing Game I got the book at my bookfair at ...   \n4          0.5    1.0  I SPY A is For Jigsaw Puzzle 63pc Hi! I'm Mart...   \n\n   Category  \n0         5  \n1         0  \n2         0  \n3         0  \n4         0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Helpfulness</th>\n      <th>Score</th>\n      <th>Text</th>\n      <th>Category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>Golden Valley Natural Buffalo Jerky The descri...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>Westing Game This was a great book!!!! It is w...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>Westing Game I am a first year teacher, teachi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>Westing Game I got the book at my bookfair at ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.5</td>\n      <td>1.0</td>\n      <td>I SPY A is For Jigsaw Puzzle 63pc Hi! I'm Mart...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"### Text cleaning\n\nFor text cleaning, you can use lower casting, punctuation removal, numbers removal, tokenization, stop words removal, stemming. This will get a perfectly cleaned text without any garbage information.","metadata":{}},{"cell_type":"code","source":"import re\n\ndef lower_text(text: str):\n    return text.lower()\n\ndef remove_numbers(text: str):\n    \"\"\"\n    Substitute all punctuations with space in case of\n    \"there is5dogs\".\n    \n    If subs with '' -> \"there isdogs\"\n    With ' ' -> there is dogs\n    \"\"\"\n    text_nonum = re.sub(r'\\d+', ' ', text)\n    return text_nonum\n\ndef remove_punctuation(text: str):\n    \"\"\"\n    Substitute all punctiations with space in case of\n    \"hello!nice to meet you\"\n    \n    If subs with '' -> \"hellonice to meet you\"\n    With ' ' -> \"hello nice to meet you\"\n    \"\"\"\n    # Write your code here\n    text_nopunct = re.sub(r'[^\\w\\s]','',text)\n    return text_nopunct\n\ndef remove_multiple_spaces(text: str):\n    # Write your code here\n    text_no_doublespace = re.sub(' +', ' ', text)\n    return text_no_doublespace","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:10:23.523454Z","iopub.execute_input":"2024-09-26T23:10:23.523961Z","iopub.status.idle":"2024-09-26T23:10:23.540185Z","shell.execute_reply.started":"2024-09-26T23:10:23.523911Z","shell.execute_reply":"2024-09-26T23:10:23.538998Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"This will give us clean text.","metadata":{}},{"cell_type":"code","source":"sample_text = train_copy['Text'][4]\n\n_lowered = lower_text(sample_text)\n_without_numbers = remove_numbers(_lowered)\n_without_punct = remove_punctuation(_without_numbers)\n_single_spaced = remove_multiple_spaces(_without_punct)\n\nprint(sample_text)\nprint('-'*10)\nprint(_lowered)\nprint('-'*10)\nprint(_without_numbers)\nprint('-'*10)\nprint(_without_punct)\nprint('-'*10)\nprint(_single_spaced)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:10:23.541846Z","iopub.execute_input":"2024-09-26T23:10:23.542260Z","iopub.status.idle":"2024-09-26T23:10:23.555486Z","shell.execute_reply.started":"2024-09-26T23:10:23.542219Z","shell.execute_reply":"2024-09-26T23:10:23.554288Z"},"trusted":true},"outputs":[{"name":"stdout","text":"I SPY A is For Jigsaw Puzzle 63pc Hi! I'm Martine Redman and I created this puzzle for Briarpatch using a great photo from Jean Marzollo and Walter Wick's terrific book, I Spy School Days. Kids need lots of practice to master the ABC's, and this puzzle provides an enjoyable reinforcing tool. Its visual richness helps non-readers and readers alike to remember word associations, and the wealth of cleverly chosen objects surrounding each letter promote language development. The riddle included multiplies the fun of assembling this colorful puzzle. For another great Briarpatch puzzle, check out I Spy Blocks. END\n----------\ni spy a is for jigsaw puzzle 63pc hi! i'm martine redman and i created this puzzle for briarpatch using a great photo from jean marzollo and walter wick's terrific book, i spy school days. kids need lots of practice to master the abc's, and this puzzle provides an enjoyable reinforcing tool. its visual richness helps non-readers and readers alike to remember word associations, and the wealth of cleverly chosen objects surrounding each letter promote language development. the riddle included multiplies the fun of assembling this colorful puzzle. for another great briarpatch puzzle, check out i spy blocks. end\n----------\ni spy a is for jigsaw puzzle  pc hi! i'm martine redman and i created this puzzle for briarpatch using a great photo from jean marzollo and walter wick's terrific book, i spy school days. kids need lots of practice to master the abc's, and this puzzle provides an enjoyable reinforcing tool. its visual richness helps non-readers and readers alike to remember word associations, and the wealth of cleverly chosen objects surrounding each letter promote language development. the riddle included multiplies the fun of assembling this colorful puzzle. for another great briarpatch puzzle, check out i spy blocks. end\n----------\ni spy a is for jigsaw puzzle  pc hi im martine redman and i created this puzzle for briarpatch using a great photo from jean marzollo and walter wicks terrific book i spy school days kids need lots of practice to master the abcs and this puzzle provides an enjoyable reinforcing tool its visual richness helps nonreaders and readers alike to remember word associations and the wealth of cleverly chosen objects surrounding each letter promote language development the riddle included multiplies the fun of assembling this colorful puzzle for another great briarpatch puzzle check out i spy blocks end\n----------\ni spy a is for jigsaw puzzle pc hi im martine redman and i created this puzzle for briarpatch using a great photo from jean marzollo and walter wicks terrific book i spy school days kids need lots of practice to master the abcs and this puzzle provides an enjoyable reinforcing tool its visual richness helps nonreaders and readers alike to remember word associations and the wealth of cleverly chosen objects surrounding each letter promote language development the riddle included multiplies the fun of assembling this colorful puzzle for another great briarpatch puzzle check out i spy blocks end\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"Now, harder preprocessing: tokenization, stop words removal and stemming.\nFor that you can use several packages, but we encourage you to use `nltk` - Natural Language ToolKit as well as `torchtext`.\n\n\nTake a look at:\n* `nltk.tokenize.word_tokenize` or `torchtext.data.utils.get_tokenizer` for tokenization\n* `nltk.corpus.stopwords` for stop words removal and `nltk.corpus.punkt` for punctuation\n* `nltk.stem.PorterStemmer` for stemming","metadata":{}},{"cell_type":"code","source":"# imports here\nimport nltk\nfrom nltk.tokenize import word_tokenize \nfrom nltk.corpus import stopwords\nfrom nltk.tokenize.punkt import PunktSentenceTokenizer\nfrom nltk.stem import PorterStemmer\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\ndef tokenize_text(text: str) -> list[str]:\n    return word_tokenize(text)\n\ndef remove_stop_words(tokenized_text: list[str]) -> list[str]:\n    filtered_sentence = [w for w in tokenized_text if not w.lower() in stop_words]\n    sent_detector = PunktSentenceTokenizer()\n    return filtered_sentence\n\ndef stem_words(tokenized_text: list[str]) -> list[str]:\n    ps = PorterStemmer()\n    return [ps.stem(t) for t in tokenized_text]","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:10:23.560030Z","iopub.execute_input":"2024-09-26T23:10:23.560466Z","iopub.status.idle":"2024-09-26T23:10:25.173142Z","shell.execute_reply.started":"2024-09-26T23:10:23.560421Z","shell.execute_reply":"2024-09-26T23:10:25.171975Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"_tokenized = tokenize_text(_single_spaced)\n_without_sw = remove_stop_words(_tokenized)\n# print(_without_sw)\n# print(_without_sw[0])\n_stemmed = stem_words(_without_sw)\n\nprint(_single_spaced)\nprint('-'*10)\nprint(_tokenized)\nprint('-'*10)\nprint(_without_sw)\nprint('-'*10)\nprint(_stemmed)\n# print(_without_sw == _stemmed)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:10:25.174690Z","iopub.execute_input":"2024-09-26T23:10:25.175179Z","iopub.status.idle":"2024-09-26T23:10:25.199185Z","shell.execute_reply.started":"2024-09-26T23:10:25.175140Z","shell.execute_reply":"2024-09-26T23:10:25.198089Z"},"trusted":true},"outputs":[{"name":"stdout","text":"i spy a is for jigsaw puzzle pc hi im martine redman and i created this puzzle for briarpatch using a great photo from jean marzollo and walter wicks terrific book i spy school days kids need lots of practice to master the abcs and this puzzle provides an enjoyable reinforcing tool its visual richness helps nonreaders and readers alike to remember word associations and the wealth of cleverly chosen objects surrounding each letter promote language development the riddle included multiplies the fun of assembling this colorful puzzle for another great briarpatch puzzle check out i spy blocks end\n----------\n['i', 'spy', 'a', 'is', 'for', 'jigsaw', 'puzzle', 'pc', 'hi', 'im', 'martine', 'redman', 'and', 'i', 'created', 'this', 'puzzle', 'for', 'briarpatch', 'using', 'a', 'great', 'photo', 'from', 'jean', 'marzollo', 'and', 'walter', 'wicks', 'terrific', 'book', 'i', 'spy', 'school', 'days', 'kids', 'need', 'lots', 'of', 'practice', 'to', 'master', 'the', 'abcs', 'and', 'this', 'puzzle', 'provides', 'an', 'enjoyable', 'reinforcing', 'tool', 'its', 'visual', 'richness', 'helps', 'nonreaders', 'and', 'readers', 'alike', 'to', 'remember', 'word', 'associations', 'and', 'the', 'wealth', 'of', 'cleverly', 'chosen', 'objects', 'surrounding', 'each', 'letter', 'promote', 'language', 'development', 'the', 'riddle', 'included', 'multiplies', 'the', 'fun', 'of', 'assembling', 'this', 'colorful', 'puzzle', 'for', 'another', 'great', 'briarpatch', 'puzzle', 'check', 'out', 'i', 'spy', 'blocks', 'end']\n----------\n['spy', 'jigsaw', 'puzzle', 'pc', 'hi', 'im', 'martine', 'redman', 'created', 'puzzle', 'briarpatch', 'using', 'great', 'photo', 'jean', 'marzollo', 'walter', 'wicks', 'terrific', 'book', 'spy', 'school', 'days', 'kids', 'need', 'lots', 'practice', 'master', 'abcs', 'puzzle', 'provides', 'enjoyable', 'reinforcing', 'tool', 'visual', 'richness', 'helps', 'nonreaders', 'readers', 'alike', 'remember', 'word', 'associations', 'wealth', 'cleverly', 'chosen', 'objects', 'surrounding', 'letter', 'promote', 'language', 'development', 'riddle', 'included', 'multiplies', 'fun', 'assembling', 'colorful', 'puzzle', 'another', 'great', 'briarpatch', 'puzzle', 'check', 'spy', 'blocks', 'end']\n----------\n['spi', 'jigsaw', 'puzzl', 'pc', 'hi', 'im', 'martin', 'redman', 'creat', 'puzzl', 'briarpatch', 'use', 'great', 'photo', 'jean', 'marzollo', 'walter', 'wick', 'terrif', 'book', 'spi', 'school', 'day', 'kid', 'need', 'lot', 'practic', 'master', 'abc', 'puzzl', 'provid', 'enjoy', 'reinforc', 'tool', 'visual', 'rich', 'help', 'nonread', 'reader', 'alik', 'rememb', 'word', 'associ', 'wealth', 'cleverli', 'chosen', 'object', 'surround', 'letter', 'promot', 'languag', 'develop', 'riddl', 'includ', 'multipli', 'fun', 'assembl', 'color', 'puzzl', 'anoth', 'great', 'briarpatch', 'puzzl', 'check', 'spi', 'block', 'end']\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"As you can see, there is a lot of words removed as well as the unnecessary language rules (I mean stems, com'on). Now we are able to construct full cleaning preprocessing stage.","metadata":{}},{"cell_type":"code","source":"def preprocessing_stage(text):\n    _lowered = lower_text(text)\n    _without_numbers = remove_numbers(_lowered)\n    _without_punct = remove_punctuation(_without_numbers)\n    _single_spaced = remove_multiple_spaces(_without_punct)\n    _tokenized = tokenize_text(_single_spaced)\n    _without_sw = remove_stop_words(_tokenized)\n    _stemmed = stem_words(_without_sw)\n    \n    return _stemmed\n\ndef clean_text_inplace(df):\n    df['Text'] = df['Text'].apply(preprocessing_stage)\n    return df\n\ndef preprocess(df):\n    df.fillna(\" \", inplace=True)\n    _preprocess_score = preprocess_score_inplace(df)\n    _preprocess_helpfulness = preprocess_helpfulness_inplace(_preprocess_score)\n    _concatted = concat_title_text_inplace(_preprocess_helpfulness)\n\n    if 'Category' in df.columns:\n        _encoded = encode_categories(_concatted)\n        _cleaned = clean_text_inplace(_encoded)\n    else:\n        _cleaned = clean_text_inplace(_concatted)\n    return _cleaned\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:10:25.200747Z","iopub.execute_input":"2024-09-26T23:10:25.201112Z","iopub.status.idle":"2024-09-26T23:10:25.211738Z","shell.execute_reply.started":"2024-09-26T23:10:25.201077Z","shell.execute_reply":"2024-09-26T23:10:25.210355Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"And now let's apply it on our train and test dataframes.","metadata":{}},{"cell_type":"code","source":"train_preprocessed = preprocess(train_dataframe)\ntest_preprocessed = preprocess(test_dataframe)\n\ntrain_preprocessed.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:10:25.213112Z","iopub.execute_input":"2024-09-26T23:10:25.213610Z","iopub.status.idle":"2024-09-26T23:11:38.245758Z","shell.execute_reply.started":"2024-09-26T23:10:25.213547Z","shell.execute_reply":"2024-09-26T23:11:38.244651Z"},"trusted":true},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"   Helpfulness  Score                                               Text  \\\n0          0.0    0.5  [golden, valley, natur, buffalo, jerki, descri...   \n1          0.0    1.0  [west, game, great, book, well, thought, easil...   \n2          0.0    1.0  [west, game, first, year, teacher, teach, th, ...   \n3          0.0    1.0  [west, game, got, book, bookfair, school, look...   \n4          0.5    1.0  [spi, jigsaw, puzzl, pc, hi, im, martin, redma...   \n\n   Category  \n0         5  \n1         0  \n2         0  \n3         0  \n4         0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Helpfulness</th>\n      <th>Score</th>\n      <th>Text</th>\n      <th>Category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>[golden, valley, natur, buffalo, jerki, descri...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>[west, game, great, book, well, thought, easil...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>[west, game, first, year, teacher, teach, th, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>[west, game, got, book, bookfair, school, look...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.5</td>\n      <td>1.0</td>\n      <td>[spi, jigsaw, puzzl, pc, hi, im, martin, redma...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"test_preprocessed.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:11:38.247132Z","iopub.execute_input":"2024-09-26T23:11:38.247473Z","iopub.status.idle":"2024-09-26T23:11:38.263795Z","shell.execute_reply.started":"2024-09-26T23:11:38.247439Z","shell.execute_reply":"2024-09-26T23:11:38.262614Z"},"trusted":true},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"   Helpfulness  Score                                               Text    ID\n0          0.0   0.75  [fhi, heat, platform, nano, technolog, work, w...  4601\n1          0.0   1.00  [boswellia, mg, cap, swanson, premium, great, ...  2554\n2          0.0   1.00  [essi, spring, collect, polish, metal, shimmer...  6181\n3          0.0   0.75  [coq, coenzym, q, pure, powder, oz, feel, im, ...  4937\n4          0.0   1.00  [halo, sleepsack, wearabl, blanket, cream, mic...  2044","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Helpfulness</th>\n      <th>Score</th>\n      <th>Text</th>\n      <th>ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.75</td>\n      <td>[fhi, heat, platform, nano, technolog, work, w...</td>\n      <td>4601</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.00</td>\n      <td>[boswellia, mg, cap, swanson, premium, great, ...</td>\n      <td>2554</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.00</td>\n      <td>[essi, spring, collect, polish, metal, shimmer...</td>\n      <td>6181</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.75</td>\n      <td>[coq, coenzym, q, pure, powder, oz, feel, im, ...</td>\n      <td>4937</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>1.00</td>\n      <td>[halo, sleepsack, wearabl, blanket, cream, mic...</td>\n      <td>2044</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"Now, let's split our original train dataset into train and val sets.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nratio = 0.2\ntrain, val = train_test_split(\n    train_preprocessed, stratify=train_preprocessed['Category'], test_size=0.2, random_state=420\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:11:38.265528Z","iopub.execute_input":"2024-09-26T23:11:38.265937Z","iopub.status.idle":"2024-09-26T23:11:38.302036Z","shell.execute_reply.started":"2024-09-26T23:11:38.265892Z","shell.execute_reply":"2024-09-26T23:11:38.300556Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"And now, for the best result, lets get rid of pandas so that nothing is stopping us from working with torchtext. For that let's create an iterator that is going to yield samples for us.","metadata":{}},{"cell_type":"markdown","source":"# Creating dataloaders\n\nFirst, you should generate our vocab from the train set.\n\nFor that, use `torchtext.vocab.build_vocab_from_iterator`.","metadata":{}},{"cell_type":"code","source":"!pip install torchtext==0.13.0","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:11:38.304128Z","iopub.execute_input":"2024-09-26T23:11:38.304622Z","iopub.status.idle":"2024-09-26T23:12:46.719220Z","shell.execute_reply.started":"2024-09-26T23:11:38.304556Z","shell.execute_reply":"2024-09-26T23:12:46.716723Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting torchtext==0.13.0\n  Downloading torchtext-0.13.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.9 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchtext==0.13.0) (4.66.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchtext==0.13.0) (2.32.3)\nCollecting torch==1.12.0 (from torchtext==0.13.0)\n  Downloading torch-1.12.0-cp310-cp310-manylinux1_x86_64.whl.metadata (22 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchtext==0.13.0) (1.26.4)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==1.12.0->torchtext==0.13.0) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.13.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.13.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.13.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.13.0) (2024.7.4)\nDownloading torchtext-0.13.0-cp310-cp310-manylinux1_x86_64.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-1.12.0-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch, torchtext\n  Attempting uninstall: torch\n    Found existing installation: torch 2.4.0+cpu\n    Uninstalling torch-2.4.0+cpu:\n      Successfully uninstalled torch-2.4.0+cpu\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 2.4.0 requires torch>=2.1.0, but you have torch 1.12.0 which is incompatible.\nstable-baselines3 2.1.0 requires torch>=1.13, but you have torch 1.12.0 which is incompatible.\ntorchaudio 2.4.0+cpu requires torch==2.4.0, but you have torch 1.12.0 which is incompatible.\ntorchvision 0.19.0+cpu requires torch==2.4.0, but you have torch 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-1.12.0 torchtext-0.13.0\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from torchtext.vocab import build_vocab_from_iterator\n\ndef yield_tokens(df):\n    for _, sample in train.iterrows():\n        yield sample.to_list()[2]\n\n# Define special symbols and indices\nUNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n# Make sure the tokens are in order of their indices to properly insert them in vocab\nspecial_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n\nvocab = build_vocab_from_iterator(yield_tokens(train), specials = special_symbols)\nvocab.set_default_index(UNK_IDX)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:12:46.722862Z","iopub.execute_input":"2024-09-26T23:12:46.723491Z","iopub.status.idle":"2024-09-26T23:12:52.754856Z","shell.execute_reply.started":"2024-09-26T23:12:46.723435Z","shell.execute_reply":"2024-09-26T23:12:52.752903Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"And then use our vocab to encode the tokenized sequence","metadata":{}},{"cell_type":"code","source":"sample = train['Text'][2]\nprint(sample)\nencoded = vocab(sample)\nprint(encoded)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:12:52.757480Z","iopub.execute_input":"2024-09-26T23:12:52.758304Z","iopub.status.idle":"2024-09-26T23:12:52.770685Z","shell.execute_reply.started":"2024-09-26T23:12:52.758258Z","shell.execute_reply":"2024-09-26T23:12:52.769566Z"},"trusted":true},"outputs":[{"name":"stdout","text":"['west', 'game', 'first', 'year', 'teacher', 'teach', 'th', 'grade', 'special', 'read', 'class', 'high', 'comprehens', 'level', 'read', 'book', 'one', 'best', 'thing', 'taught', 'year', 'expand', 'mind', 'allow', 'put', 'charact', 'place', 'easi', 'student', 'make', 'mind', 'movi', 'even', 'use', 'whole', 'read', 'class', 'time', 'order', 'finish', 'book', 'student', 'couldnt', 'wait', 'hear', 'end', 'excel', 'book', 'read', 'everi', 'year', 'student']\n[2569, 43, 34, 15, 2794, 797, 851, 1761, 724, 136, 1928, 211, 7068, 595, 136, 534, 5, 63, 46, 3511, 15, 2915, 540, 444, 40, 1136, 171, 52, 1916, 21, 540, 958, 30, 4, 273, 136, 1928, 13, 71, 633, 534, 1916, 356, 429, 598, 185, 321, 534, 136, 87, 15, 1916]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"Now we can define our collate function and create dataloaders","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\n\ntorch.manual_seed(420)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef collate_batch(batch):\n    label_list, text_list, score_list, helpfulness_list, offsets = [], [], [], [], [0]\n    for _helpfulnes, _score, _text, _label in batch:\n        label_list.append(_label)\n        processed_text = torch.tensor(vocab(_text), dtype=torch.int64)\n        text_list.append(processed_text)\n        score_list.append(_score)\n        helpfulness_list.append(_helpfulnes)\n        offsets.append(processed_text.size(0))\n\n    label_list = torch.tensor(label_list, dtype=torch.int64)\n    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n    text_list = torch.cat(text_list)\n    score_list = torch.tensor(score_list, dtype=torch.float64)\n    helpfulness_list = torch.tensor(helpfulness_list, dtype=torch.float64)\n        \n    return label_list.to(device), text_list.to(device), offsets.to(device), score_list.to(device), helpfulness_list.to(device)\n\ntrain_dataloader = DataLoader(\n    train.to_numpy(), batch_size=128, shuffle=True, collate_fn=collate_batch\n)\n\nval_dataloader = DataLoader(\n    val.to_numpy(), batch_size=128, shuffle=False, collate_fn=collate_batch\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:12:52.772261Z","iopub.execute_input":"2024-09-26T23:12:52.772718Z","iopub.status.idle":"2024-09-26T23:12:54.971959Z","shell.execute_reply.started":"2024-09-26T23:12:52.772665Z","shell.execute_reply":"2024-09-26T23:12:54.970667Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Defining Network\n\n\nFor writing a network you can use `torch.nn.Embedding` or `torch.nn.EmbeddingBag`. This will allow your netorwk to learn embedding vector for your tokens.\n\nAs for the other modules in your network, consider these options:\n* Simple Linear layers, activations, basic stuff that goes into the network\n* There is a possible of not using the offsets (indices of sequences) in the formart, put use predefined sequence length (maximum length, some value, etc.). If this is an option for you, change the `collate_batch` function according to your architecture.\n* You could use all this recurrent stuff (RNN, GRU, LSTM, even Transformer, all up to you), but remembder about the dimentions and hidden states","metadata":{}},{"cell_type":"code","source":"# import torch.nn as nn\n# class TextClassificationModel(nn.Module):\n#     def __init__(self, vocab_size, embed_dim, num_class):\n#         super(TextClassificationModel, self).__init__()\n#         self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n#         self.fc = nn.Linear(embed_dim, num_class)\n#         self.init_weights()\n\n#     def init_weights(self):\n#         initrange = 0.5\n#         self.embedding.weight.data.uniform_(-initrange, initrange)\n#         self.fc.weight.data.uniform_(-initrange, initrange)\n#         self.fc.bias.data.zero_()\n\n#     def forward(self, text, offsets, scores, helpfulnesses):\n#         embedded = self.embedding(text, offsets, scores, helpfulnesses)\n#         return self.fc(embedded)\n# model = TextClassificationModel(len(vocab), 128, 6)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:12:54.974187Z","iopub.execute_input":"2024-09-26T23:12:54.974675Z","iopub.status.idle":"2024-09-26T23:12:55.134072Z","shell.execute_reply.started":"2024-09-26T23:12:54.974630Z","shell.execute_reply":"2024-09-26T23:12:55.132760Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import torch.nn as nn\nclass TextClassificationModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_class):\n        super(TextClassificationModel, self).__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n        # Adjust the input dimension of the linear layer to account for the additional features\n        self.fc = nn.Linear(embed_dim + 2, num_class)  # +2 for scores and helpfulnesses\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.5\n        self.embedding.weight.data.uniform_(-initrange, initrange)\n        self.fc.weight.data.uniform_(-initrange, initrange)\n        self.fc.bias.data.zero_()\n\n    def forward(self, text, offsets, scores, helpfulnesses):\n        # Ensure the input tensors are float32\n        scores = scores.float()  # Convert scores to float32\n        helpfulnesses = helpfulnesses.float()  # Convert helpfulnesses to float32\n\n        embedded = self.embedding(text, offsets)  # Get embedded text\n        # Concatenate scores and helpfulnesses as extra features\n        extra_features = torch.cat((scores.unsqueeze(1), helpfulnesses.unsqueeze(1)), dim=1)\n        # Concatenate the embedded text with the extra features\n        combined = torch.cat((embedded, extra_features), dim=1)\n        return self.fc(combined)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:12:55.135793Z","iopub.execute_input":"2024-09-26T23:12:55.136322Z","iopub.status.idle":"2024-09-26T23:12:55.257524Z","shell.execute_reply.started":"2024-09-26T23:12:55.136270Z","shell.execute_reply":"2024-09-26T23:12:55.256037Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm.autonotebook import tqdm\nimport torch\n\ndef train_one_epoch(\n    model,\n    loader,\n    optimizer,\n    loss_fn,\n    epoch_num=-1\n):\n    loop = tqdm(\n        enumerate(loader, 1),\n        total=len(loader),\n        desc=f\"Epoch {epoch_num}: train\",\n        leave=True,\n    )\n    model.train()\n    train_loss = 0.0\n    for i, batch in loop:\n        labels, texts, offsets, scores, helpfulness = batch\n        # Move inputs and labels to the device (GPU/CPU)\n        labels = labels.to(device)\n        texts = texts.to(device)\n        offsets = offsets.to(device)\n        helpfulness = helpfulness.to(device)\n        scores = scores.to(device)\n        # forward pass\n\n        \n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward pass\n        outputs = model(texts, offsets, helpfulness, scores)# Assuming the model takes `texts` as input\n\n        # loss calculation\n        loss = loss_fn(outputs, labels)\n        \n        # backward pass\n        loss.backward()\n\n        # optimizer step\n        optimizer.step()\n\n        train_loss += loss.item() * len(labels)  # sum of losses (scaled by batch size)\n        loop.set_postfix({\"loss\": train_loss / (i * len(labels))})  # avg loss so far\n\ndef val_one_epoch(\n    model,\n    loader,\n    loss_fn,\n    epoch_num=-1,\n    best_so_far=0.0,\n    ckpt_path='best.pt'\n):\n    loop = tqdm(\n        enumerate(loader, 1),\n        total=len(loader),\n        desc=f\"Epoch {epoch_num}: val\",\n        leave=True,\n    )\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        model.eval()  # evaluation mode\n        for i, batch in loop:\n            labels, texts, offsets, scores, helpfulness = batch\n\n            # Move inputs and labels to the device\n            labels = labels.to(device)\n            texts = texts.to(device)\n            offsets = offsets.to(device)\n            helpfulness = helpfulness.to(device)\n            scores = scores.to(device)\n            # forward pass\n            outputs = model(texts, offsets, helpfulness, scores)\n\n            # loss calculation\n            loss = loss_fn(outputs, labels)\n            \n            # get predictions and accumulate correct predictions\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n            val_loss += loss.item() * len(labels)  # sum of losses (scaled by batch size)\n            loop.set_postfix({\"loss\": val_loss / total, \"acc\": correct / total})\n\n        # Check if current accuracy is better than the best so far\n        val_acc = correct / total\n        if val_acc > best_so_far:\n            torch.save(model.state_dict(), ckpt_path)  # save the model's state_dict\n            return val_acc  # return new best accuracy\n\n    return best_so_far  # if no improvement, return the previous best accuracy\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:12:55.259296Z","iopub.execute_input":"2024-09-26T23:12:55.260113Z","iopub.status.idle":"2024-09-26T23:12:55.403093Z","shell.execute_reply.started":"2024-09-26T23:12:55.260041Z","shell.execute_reply":"2024-09-26T23:12:55.401385Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import torch.optim as optim\nepochs = 10  # Number of epochs to train\nvocab_size = len(vocab) \nembed_dim = 128  # Size of the embedding\nnum_class = 6\n# Instantiate the model, optimizer, and loss function\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = TextClassificationModel(vocab_size, embed_dim, num_class).to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)  # Using Adam optimizer\nloss_fn = nn.CrossEntropyLoss()  ","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:15:47.622264Z","iopub.execute_input":"2024-09-26T23:15:47.622734Z","iopub.status.idle":"2024-09-26T23:15:47.768757Z","shell.execute_reply.started":"2024-09-26T23:15:47.622687Z","shell.execute_reply":"2024-09-26T23:15:47.767460Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"best = -float('inf')\nfor epoch in range(epochs):\n    train_one_epoch(model, train_dataloader, optimizer, loss_fn, epoch_num=epoch)\n    best = val_one_epoch(model, val_dataloader, loss_fn, epoch, best_so_far=best)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:15:49.334144Z","iopub.execute_input":"2024-09-26T23:15:49.334607Z","iopub.status.idle":"2024-09-26T23:15:59.018528Z","shell.execute_reply.started":"2024-09-26T23:15:49.334551Z","shell.execute_reply":"2024-09-26T23:15:59.016555Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 0: train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c14b756877ad47308a89d5c59659656d"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     best \u001b[38;5;241m=\u001b[39m val_one_epoch(model, val_dataloader, loss_fn, epoch, best_so_far\u001b[38;5;241m=\u001b[39mbest)\n","Cell \u001b[0;32mIn[22], line 43\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, loss_fn, epoch_num)\u001b[0m\n\u001b[1;32m     40\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# optimizer step\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)  \u001b[38;5;66;03m# sum of losses (scaled by batch size)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_loss \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels))})\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:307\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    305\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":26},{"cell_type":"markdown","source":"# Predictions","metadata":{}},{"cell_type":"code","source":"test_preprocessed.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:15:59.019568Z","iopub.status.idle":"2024-09-26T23:15:59.020041Z","shell.execute_reply.started":"2024-09-26T23:15:59.019831Z","shell.execute_reply":"2024-09-26T23:15:59.019854Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_batch(batch):\n    text_list, score_list, helpfulness_list, offsets =  [], [], [], [0]\n    for _helpfulnes, _score, _text, ids in batch:\n        processed_text = torch.tensor(vocab(_text), dtype=torch.int64)\n        text_list.append(processed_text)\n        score_list.append(_score)\n        helpfulness_list.append(_helpfulnes)\n        offsets.append(processed_text.size(0))\n\n    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n    text_list = torch.cat(text_list)\n    score_list = torch.tensor(score_list, dtype=torch.float64)\n    helpfulness_list = torch.tensor(helpfulness_list, dtype=torch.float64)\n    return text_list.to(device), offsets.to(device), score_list.to(device), helpfulness_list.to(device)\n#     return text_list, offsets, score_list, helpfulness_list\n\ntest_dataloader = DataLoader(\n    test_preprocessed.to_numpy(), batch_size=128, shuffle=False, collate_fn=collate_batch\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:12:56.384026Z","iopub.status.idle":"2024-09-26T23:12:56.387526Z","shell.execute_reply.started":"2024-09-26T23:12:56.386342Z","shell.execute_reply":"2024-09-26T23:12:56.386465Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(\n    model,\n    loader,\n):\n    loop = tqdm(\n        enumerate(loader, 1),\n        total=len(loader),\n        desc=\"Predictions:\",\n        leave=True,\n    )\n    predictions = []\n    with torch.no_grad():\n        model.eval()  # evaluation mode\n        for i, batch in loop:\n            texts, offsets, scores, helpfulness = batch\n\n            # forward pass and loss calculation\n            outputs = model(texts, offsets, helpfulness, scores)\n            \n            _, predicted = torch.max(outputs.data, 1)\n            predictions += predicted.detach().cpu().tolist()\n\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:12:56.391005Z","iopub.status.idle":"2024-09-26T23:12:56.393230Z","shell.execute_reply.started":"2024-09-26T23:12:56.392537Z","shell.execute_reply":"2024-09-26T23:12:56.392683Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ckpt = torch.load(\"best.pt\")\nmodel.load_state_dict(ckpt)\n\npredictions = predict(model, test_dataloader)\npredictions[:10]","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:12:56.396308Z","iopub.status.idle":"2024-09-26T23:12:56.398378Z","shell.execute_reply.started":"2024-09-26T23:12:56.397302Z","shell.execute_reply":"2024-09-26T23:12:56.397413Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df = test_dataframe.copy()\n\nsubmission_df.drop(['Helpfulness','Score','Text'],axis=1,inplace=True)\nsubmission_df['Category'] = [idx2cat[x] for x in predictions]\n\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:12:56.401283Z","iopub.status.idle":"2024-09-26T23:12:56.402663Z","shell.execute_reply.started":"2024-09-26T23:12:56.402026Z","shell.execute_reply":"2024-09-26T23:12:56.402060Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}
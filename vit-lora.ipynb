{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":87115,"databundleVersionId":9889138,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this lab you're asking to fine tune a Visual Transformer classifier on target dataset\n\n\n\nObjectives:\n\n\n\n1) Get familiar with **Huggingface** - the main library for working with transformers;\n\n\n\n2) Use **low-rank adapters** for cheap training of a transformer.","metadata":{"id":"JwhYBu5TobO5"}},{"cell_type":"markdown","source":"### 1) Load transformers packages & dataset\n\n\n\n***Transformers*** - is a package which is assosiated with HuggingFace community. It allows to load (and push) trained transformers and datasets. *transformers* package also connects with pytorch which allows to train a model by your own.\n\n\n\nWe will load an Visual Transformer (ViT) that was trained on ImageNet and fine tune it on images with different foods.","metadata":{"id":"yOptj0ZbwnwE"}},{"cell_type":"code","source":"!pip install transformers accelerate evaluate datasets git+https://github.com/huggingface/peft -q","metadata":{"id":"0Q69QetVg_-W","outputId":"d095cac8-0374-419a-aa93-d768986caf99","execution":{"iopub.status.busy":"2024-10-24T22:43:04.557167Z","iopub.execute_input":"2024-10-24T22:43:04.557982Z","iopub.status.idle":"2024-10-24T22:43:32.868404Z","shell.execute_reply.started":"2024-10-24T22:43:04.557924Z","shell.execute_reply":"2024-10-24T22:43:32.867100Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\nimport torch\n\nfrom datasets import load_from_disk\n\n\nfrom transformers import AutoImageProcessor\n","metadata":{"id":"vTdGbbCLjqNG","execution":{"iopub.status.busy":"2024-10-24T22:43:32.873906Z","iopub.execute_input":"2024-10-24T22:43:32.874297Z","iopub.status.idle":"2024-10-24T22:43:51.580756Z","shell.execute_reply.started":"2024-10-24T22:43:32.874251Z","shell.execute_reply":"2024-10-24T22:43:51.579910Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Let's implement some preprocessing functions to fit images to ViT shape and distribution and add some augmentation","metadata":{"id":"LbfsLlF8xHbK"}},{"cell_type":"code","source":"# Source director\nimport shutil\nsource_dir = \"/kaggle/input/pmldl-week-8-fine-tuning-of-vi-t\"\n\n# Destination directory\ndestination_dir = \"/kaggle/working/tr\"\ntry:\n    \n    shutil.copytree(source_dir, destination_dir)\nexcept:\n    print(\"dir already existing\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T22:43:51.582027Z","iopub.execute_input":"2024-10-24T22:43:51.582797Z","iopub.status.idle":"2024-10-24T22:43:59.491929Z","shell.execute_reply.started":"2024-10-24T22:43:51.582750Z","shell.execute_reply":"2024-10-24T22:43:59.490892Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Add augmentation procedures if you like\n\nfrom torchvision.transforms import Normalize, ToTensor\n\n\n\nfrom torchvision.transforms import v2\n\n# Target dataset\n\ndataset = load_from_disk(\"/kaggle/working/tr/food-101_train/food-101_train\")\n\n\n\n# Data prepapator for a model\n\nimage_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\", use_fast=True)\n\n\n\n# Extract parameters from image_processor\n\n# Write your code here\n\n# normalize = Normalize(mean=..., std=...)\n\nnormalize = Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n\n# Write your code here\n\n# Note that the size of images should fit size of image_processor\n\n# train_transforms = Compose(\n\n#     [\n\n#         ...\n\n#         ToTensor(),\n\n#         normalize,\n\n#     ]\n\n# )\n\ntransforms = v2.Compose([\n\n    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n\n    v2.RandomHorizontalFlip(p=0.5),\n\n    # v2.ToDtype(torch.float32, scale=True),\n\n    \n    ToTensor(),\n\n    normalize,\n\n])\nvalidate_transform = v2.Compose([\n\n    v2.Resize(size=(256, 256), antialias=True),  # Resize the shorter side to 256 pixels\n\n    v2.CenterCrop(size=(224, 224)),  # Crop the center to match model input size\n\n    ToTensor(),  # Convert to tensor\n\n    normalize  # Apply the same normalization used during training\n])\n\n# Write your code here\n\n# Note that the size of images should fit size of image_processor\n\n# val_transforms = Compose(\n\n#     [\n\n#         ...\n\n#         ToTensor(),\n\n#         normalize,\n\n#     ]\n\n# )\n\n\n\ndef preprocess_train(example_batch):\n\n    \"\"\"Apply train_transforms across a batch.\"\"\"\n\n    example_batch[\"pixel_values\"] = [transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n\n    return example_batch\n\n\n\n\n\ndef preprocess_val(example_batch):\n\n    \"\"\"Apply val_transforms across a batch.\"\"\"\n\n    example_batch[\"pixel_values\"] = [validate_transform(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n\n    return example_batch","metadata":{"id":"vY4xb2jijvV6","outputId":"3538df03-bfb4-4d97-edea-97145abcddf2","execution":{"iopub.status.busy":"2024-10-24T22:43:59.494682Z","iopub.execute_input":"2024-10-24T22:43:59.495006Z","iopub.status.idle":"2024-10-24T22:44:00.145582Z","shell.execute_reply.started":"2024-10-24T22:43:59.494973Z","shell.execute_reply":"2024-10-24T22:44:00.144615Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"917d5b5cafb540f89478539b9c3810ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c3bcd28529a4d3983d9611c1ccbf7c6"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"Next, we need to map labels from string to int and vise versa","metadata":{"id":"iX4u51BCyRTX"}},{"cell_type":"code","source":"label2id, id2label = dict(), dict()\n\nlabels = dataset.features[\"label\"].names\n\n\n\n# Go through the labels and save corresponding indexes\n\nfor i, label in enumerate(labels):\n\n    label2id[label] = i  # map label to id\n\n    id2label[i] = label  # map id to label\n","metadata":{"id":"nhgnzsBdjxjp","execution":{"iopub.status.busy":"2024-10-24T22:44:00.146752Z","iopub.execute_input":"2024-10-24T22:44:00.147062Z","iopub.status.idle":"2024-10-24T22:44:00.153438Z","shell.execute_reply.started":"2024-10-24T22:44:00.147028Z","shell.execute_reply":"2024-10-24T22:44:00.152354Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"Do train-test split","metadata":{"id":"W19i7xMV0sOK"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split up training into training + validation\n\nsplits = dataset.train_test_split(test_size=0.1)\n\ntrain_ds = splits[\"train\"]\n\nval_ds = splits[\"test\"]\n\n\n\ntrain_ds.set_transform(preprocess_train)\n\nval_ds.set_transform(preprocess_val)","metadata":{"id":"Dv5W5JnEkMZ4","execution":{"iopub.status.busy":"2024-10-24T22:44:00.154612Z","iopub.execute_input":"2024-10-24T22:44:00.154901Z","iopub.status.idle":"2024-10-24T22:44:00.281225Z","shell.execute_reply.started":"2024-10-24T22:44:00.154869Z","shell.execute_reply":"2024-10-24T22:44:00.280483Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### 2) Model loading\n\n\n\nFirst of all, we should load the model itself","metadata":{"id":"brT1U7S1yarn"}},{"cell_type":"code","source":"def print_trainable_parameters(model):\n\n    \"\"\"\n\n    Prints the number of trainable parameters in the model.\n\n    \"\"\"\n\n    trainable_params = 0\n\n    all_param = 0\n\n    for _, param in model.named_parameters():\n\n        all_param += param.numel()\n\n        if param.requires_grad:\n\n            trainable_params += param.numel()\n\n    print(\n\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n\n    )","metadata":{"id":"UdeMVYgTaoAU","execution":{"iopub.status.busy":"2024-10-24T22:44:00.282621Z","iopub.execute_input":"2024-10-24T22:44:00.282924Z","iopub.status.idle":"2024-10-24T22:44:00.287977Z","shell.execute_reply.started":"2024-10-24T22:44:00.282892Z","shell.execute_reply":"2024-10-24T22:44:00.287106Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n\n\n\n# Write your code here\n\nmodel = AutoModelForImageClassification.from_pretrained(\n\n    \"google/vit-base-patch16-224\",\n\n    label2id=label2id,\n\n    id2label=id2label,\n\n    ignore_mismatched_sizes=True\n\n)\n\nprint_trainable_parameters(model)","metadata":{"id":"BUxnNGspaqyX","outputId":"c218d3b6-7605-4d2b-e754-cc57e9306093","execution":{"iopub.status.busy":"2024-10-24T22:44:00.289160Z","iopub.execute_input":"2024-10-24T22:44:00.289468Z","iopub.status.idle":"2024-10-24T22:44:04.655017Z","shell.execute_reply.started":"2024-10-24T22:44:00.289437Z","shell.execute_reply":"2024-10-24T22:44:04.654197Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c41e4a59f72444b7a3269b09e631c6c1"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([101]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([101, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 85876325 || all params: 85876325 || trainable%: 100.00\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### 3) Low-rank adaptation\n\n\n\n[LoRA](https://arxiv.org/pdf/2106.09685) - is a well-known method for transformers training. The one can **decompose** weight matrix of a transformer into two smaller matricies.\n\n\n\nWhere are several parameters for LoRA. For now, let's focus on one, **r** - intrictic dimension of the decomposed matricies. **r** usually varies from 4 to 64.","metadata":{"id":"7dJViBmn163V"}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\n# Load config\n\n# Write your code here\n\nconfig = LoraConfig(\n\n    r=32,\n\n    lora_alpha=16,\n\n    target_modules=[\"query\", \"value\"],\n\n    lora_dropout=0.1,\n\n    bias=\"none\",\n\n    modules_to_save=[\"classifier\"],\n\n)\n\nlora_model = get_peft_model(model, config)\n\nprint_trainable_parameters(lora_model)\n","metadata":{"id":"SU7DyaA2ayfe","outputId":"7cd0cf25-d7a8-4ae2-a97d-51b1860a0c6e","execution":{"iopub.status.busy":"2024-10-24T22:44:04.656295Z","iopub.execute_input":"2024-10-24T22:44:04.657093Z","iopub.status.idle":"2024-10-24T22:44:04.727677Z","shell.execute_reply.started":"2024-10-24T22:44:04.657043Z","shell.execute_reply":"2024-10-24T22:44:04.726775Z"},"trusted":true},"outputs":[{"name":"stdout","text":"trainable params: 1257317 || all params: 87133642 || trainable%: 1.44\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"That's how you prepared an adapter. Note the trainable percent of parameters","metadata":{"id":"plv6xM_m9mWo"}},{"cell_type":"markdown","source":"### 4) Training of transformer\n\n\n\nFor `transformers` you don't need to write a training function as in pytorch. You need to set all the training config in `TrainingArguments` and run a `Trainer`.\n\n\n","metadata":{"id":"DhgJvAui9oX4"}},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\n\n\n# Write your code here\n\nbatch_size = 64\n\nepochs = 8\n\n# Train LoRA and save it to \"fine-tunned-model\"\n\nargs = TrainingArguments(\n\n    \"fine-tunned-model\",\n\n    remove_unused_columns=False,\n\n    eval_strategy=\"epoch\",\n\n    save_strategy=\"epoch\",\n\n    learning_rate=5e-3,\n\n    per_device_train_batch_size=batch_size,\n\n    gradient_accumulation_steps=4,\n\n    per_device_eval_batch_size=batch_size,\n\n    fp16=True,\n\n    num_train_epochs=epochs,\n\n    logging_steps=10,\n\n    load_best_model_at_end=True,\n\n    metric_for_best_model=\"accuracy\",\n\n    push_to_hub=False,\n\n    label_names=[\"labels\"],\n    report_to=None\n)","metadata":{"id":"7S97rMw-a4UY","execution":{"iopub.status.busy":"2024-10-24T22:44:04.729045Z","iopub.execute_input":"2024-10-24T22:44:04.729894Z","iopub.status.idle":"2024-10-24T22:44:04.828500Z","shell.execute_reply.started":"2024-10-24T22:44:04.729845Z","shell.execute_reply":"2024-10-24T22:44:04.827335Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"Let's define a function for performance calculation and collate function that will map a sample from dataset into the image and label","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nimport evaluate\n\nimport torch\n\n\n\nmetric = evaluate.load(\"accuracy\")\n\n\n\n# the compute_metrics function takes a Named Tuple as input:\n\n# predictions, which are the logits of the model as Numpy arrays,\n\n# and label_ids, which are the ground-truth labels as Numpy arrays.\n\n# Use metric.compute(...) to calculate an accuracy between arrays\ndef compute_metrics(eval_pred):\n    # Unpack the predictions and true labels\n    logits, label_ids = eval_pred\n    \n    # Convert logits to predicted class indices (the class with the highest score)\n    predictions = np.argmax(logits, axis=1)\n    \n    # Calculate the accuracy using the metric defined earlier\n    accuracy = metric.compute(predictions=predictions, references=label_ids)\n    \n    return accuracy\n\n\ndef collate_fn(examples):\n\n    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n\n    labels = torch.tensor([example[\"label\"] for example in examples])\n\n    return {\"pixel_values\": pixel_values, \"labels\": labels}","metadata":{"id":"zfdnPXfrbBzM","execution":{"iopub.status.busy":"2024-10-24T22:44:04.833665Z","iopub.execute_input":"2024-10-24T22:44:04.834454Z","iopub.status.idle":"2024-10-24T22:44:05.559018Z","shell.execute_reply.started":"2024-10-24T22:44:04.834408Z","shell.execute_reply":"2024-10-24T22:44:05.558261Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f68a00fe3c694d43851d2eb2de37735a"}},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"Define the main training function:","metadata":{}},{"cell_type":"code","source":"import gc\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T22:44:05.560097Z","iopub.execute_input":"2024-10-24T22:44:05.560407Z","iopub.status.idle":"2024-10-24T22:44:05.868152Z","shell.execute_reply.started":"2024-10-24T22:44:05.560373Z","shell.execute_reply":"2024-10-24T22:44:05.867260Z"},"trusted":true},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"210"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import torch\n\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T22:44:05.871655Z","iopub.execute_input":"2024-10-24T22:44:05.872012Z","iopub.status.idle":"2024-10-24T22:44:05.879822Z","shell.execute_reply.started":"2024-10-24T22:44:05.871977Z","shell.execute_reply":"2024-10-24T22:44:05.878718Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-10-24T22:44:05.881070Z","iopub.execute_input":"2024-10-24T22:44:05.881441Z","iopub.status.idle":"2024-10-24T22:44:06.931692Z","shell.execute_reply.started":"2024-10-24T22:44:05.881395Z","shell.execute_reply":"2024-10-24T22:44:06.930696Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Thu Oct 24 22:44:06 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   39C    P0             27W /  250W |       3MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!pip install wandb","metadata":{"id":"5MlxkBve-lxr","execution":{"iopub.status.busy":"2024-10-24T22:44:06.933268Z","iopub.execute_input":"2024-10-24T22:44:06.933651Z","iopub.status.idle":"2024-10-24T22:44:18.693286Z","shell.execute_reply.started":"2024-10-24T22:44:06.933609Z","shell.execute_reply":"2024-10-24T22:44:18.692299Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.18.3)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.15.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (70.0.0)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import wandb\n\ntry:\n    \n    wandb.login(key='1a7158562f4e89adb744a65c610197888f19516b')\n    anony = None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')","metadata":{"execution":{"iopub.status.busy":"2024-10-24T22:52:37.631105Z","iopub.execute_input":"2024-10-24T22:52:37.631471Z","iopub.status.idle":"2024-10-24T22:52:37.837603Z","shell.execute_reply.started":"2024-10-24T22:52:37.631437Z","shell.execute_reply":"2024-10-24T22:52:37.836743Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myazan-nukari\u001b[0m (\u001b[33myazan-nukari-innopolis-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"trainer = Trainer(\n\n    lora_model,\n\n    args,\n\n    train_dataset=train_ds,\n\n    eval_dataset=val_ds,\n\n    tokenizer=image_processor,\n\n    compute_metrics=compute_metrics,\n\n    data_collator=collate_fn,\n\n)\n\ntrain_results = trainer.train()","metadata":{"id":"oNDCt6frg9MJ","outputId":"d246eba6-72c5-4ee2-9f8c-2cfaba9495d7","execution":{"iopub.status.busy":"2024-10-24T22:52:38.332887Z","iopub.execute_input":"2024-10-24T22:52:38.333683Z","iopub.status.idle":"2024-10-24T22:52:45.768968Z","shell.execute_reply.started":"2024-10-24T22:52:38.333638Z","shell.execute_reply":"2024-10-24T22:52:45.766311Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112961177776824, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d15cd67de5374502ab3af66ba889c019"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241024_225238-fzute4e2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/yazan-nukari-innopolis-university/huggingface/runs/fzute4e2' target=\"_blank\">fine-tunned-model</a></strong> to <a href='https://wandb.ai/yazan-nukari-innopolis-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/yazan-nukari-innopolis-university/huggingface' target=\"_blank\">https://wandb.ai/yazan-nukari-innopolis-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/yazan-nukari-innopolis-university/huggingface/runs/fzute4e2' target=\"_blank\">https://wandb.ai/yazan-nukari-innopolis-university/huggingface/runs/fzute4e2</a>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 19\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m     lora_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2345\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2342\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2344\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2345\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2346\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:561\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[0;32m--> 561\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:2746\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2744\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: List) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[1;32m   2745\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2746\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2747\u001b[0m     n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[1;32m   2748\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:2742\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2740\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2741\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:2727\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2725\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2726\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[0;32m-> 2727\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   2729\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:639\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    637\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:407\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:521\u001b[0m, in \u001b[0;36mCustomFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    520\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_batch(pa_table)\n\u001b[0;32m--> 521\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_features_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(batch)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:228\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m batch\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/features/features.py:2084\u001b[0m, in \u001b[0;36mFeatures.decode_batch\u001b[0;34m(self, batch, token_per_repo_id)\u001b[0m\n\u001b[1;32m   2081\u001b[0m decoded_batch \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2083\u001b[0m     decoded_batch[column_name] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 2084\u001b[0m         [\n\u001b[1;32m   2085\u001b[0m             decode_nested_example(\u001b[38;5;28mself\u001b[39m[column_name], value, token_per_repo_id\u001b[38;5;241m=\u001b[39mtoken_per_repo_id)\n\u001b[1;32m   2086\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2087\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2088\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m column\n\u001b[1;32m   2089\u001b[0m         ]\n\u001b[1;32m   2090\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[1;32m   2091\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[1;32m   2092\u001b[0m     )\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_batch\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/features/features.py:2085\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2081\u001b[0m decoded_batch \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2083\u001b[0m     decoded_batch[column_name] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2084\u001b[0m         [\n\u001b[0;32m-> 2085\u001b[0m             \u001b[43mdecode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2086\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2087\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2088\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m column\n\u001b[1;32m   2089\u001b[0m         ]\n\u001b[1;32m   2090\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[1;32m   2091\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[1;32m   2092\u001b[0m     )\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_batch\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/features/features.py:1403\u001b[0m, in \u001b[0;36mdecode_nested_example\u001b[0;34m(schema, obj, token_per_repo_id)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (Audio, Image)):\n\u001b[1;32m   1401\u001b[0m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m schema\u001b[38;5;241m.\u001b[39mdecode:\n\u001b[0;32m-> 1403\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/features/image.py:188\u001b[0m, in \u001b[0;36mImage.decode_example\u001b[0;34m(self, value, token_per_repo_id)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(BytesIO(bytes_))\n\u001b[0;32m--> 188\u001b[0m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# to avoid \"Too many open files\" errors\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mgetexif()\u001b[38;5;241m.\u001b[39mget(PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mExifTags\u001b[38;5;241m.\u001b[39mBase\u001b[38;5;241m.\u001b[39mOrientation) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImageOps\u001b[38;5;241m.\u001b[39mexif_transpose(image)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/ImageFile.py:293\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    292\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 293\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":25},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from peft import PeftModel\n\ntrainer.model.save_pretrained(\"my_adapter\")\n\n\n\nfinetuned_model = PeftModel.from_pretrained(model,\n\n                                  \"my_adapter\",\n\n                                  torch_dtype=torch.float16,\n\n                                  is_trainable=False,\n\n                                  device_map=\"auto\"\n\n                                  )\n\nfinetuned_model = finetuned_model.merge_and_unload()","metadata":{"id":"f4PY3EOWpWGz","execution":{"iopub.status.busy":"2024-10-24T22:51:36.870648Z","iopub.status.idle":"2024-10-24T22:51:36.871060Z","shell.execute_reply.started":"2024-10-24T22:51:36.870866Z","shell.execute_reply":"2024-10-24T22:51:36.870888Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test dataset\n\n\n\ntest_dataset = load_from_disk(\"/kaggle/working/tr/food-101_test_images/food-101_test_images\")\n\ntest_dataset.set_transform(preprocess_val)","metadata":{"id":"MqTv_1yzlRu3","execution":{"iopub.status.busy":"2024-10-24T22:51:36.873017Z","iopub.status.idle":"2024-10-24T22:51:36.873408Z","shell.execute_reply.started":"2024-10-24T22:51:36.873223Z","shell.execute_reply":"2024-10-24T22:51:36.873244Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-24T22:51:36.874856Z","iopub.status.idle":"2024-10-24T22:51:36.875219Z","shell.execute_reply.started":"2024-10-24T22:51:36.875037Z","shell.execute_reply":"2024-10-24T22:51:36.875057Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(examples):\n    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n    # labels = torch.tensor([example[\"label\"] for example in examples])  # This line is not needed for test data\n    return {\"pixel_values\": pixel_values}\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T22:51:36.876965Z","iopub.status.idle":"2024-10-24T22:51:36.877336Z","shell.execute_reply.started":"2024-10-24T22:51:36.877152Z","shell.execute_reply":"2024-10-24T22:51:36.877172Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\n\n# Load the fine-tuned model (assuming it has already been merged and unloaded)\nfinetuned_model.eval()  # Set the model to evaluation mode\n\n# DataLoader to efficiently batch and make predictions\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n\n# Initialize a list to store the predictions and image ids\npredictions_list = []\nimage_id =0 \n# Disable gradient calculation to save memory and computations\nwith torch.no_grad():\n    for batch in test_loader:\n        # Move the pixel values to the same device as the model\n        pixel_values = batch[\"pixel_values\"].to(finetuned_model.device)\n\n        # Forward pass through the model to get logits\n        outputs = finetuned_model(pixel_values)\n        logits = outputs.logits\n\n        # Get the predicted labels (class with the highest score)\n        predicted_labels = torch.argmax(logits, dim=-1).cpu().numpy()\n\n        # Collect image IDs or other identifying information (assuming test_dataset has 'id' field)\n        for pred in predicted_labels:\n            predictions_list.append({\"ID\": image_id, \"TARGET\": id2label[pred]})\n            image_id+=1\n\n# Create a DataFrame to store predictions\npredictions_df = pd.DataFrame(predictions_list)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T22:51:36.879259Z","iopub.status.idle":"2024-10-24T22:51:36.879672Z","shell.execute_reply.started":"2024-10-24T22:51:36.879450Z","shell.execute_reply":"2024-10-24T22:51:36.879469Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Save predictions to a CSV file\npredictions_df.to_csv(\"submission.csv\", index=False)\n\nprint(\"Predictions saved to submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T22:51:36.881248Z","iopub.status.idle":"2024-10-24T22:51:36.881641Z","shell.execute_reply.started":"2024-10-24T22:51:36.881428Z","shell.execute_reply":"2024-10-24T22:51:36.881448Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions_df","metadata":{"execution":{"iopub.status.busy":"2024-10-24T22:51:36.883187Z","iopub.status.idle":"2024-10-24T22:51:36.883583Z","shell.execute_reply.started":"2024-10-24T22:51:36.883379Z","shell.execute_reply":"2024-10-24T22:51:36.883398Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}